# Pipeline de ML
## 
Se van a utilizar los datos de la competencia de kaggle "House Prices - Advanced Regression Techniques https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview
Competition Description
![housesbanner](https://user-images.githubusercontent.com/2281529/134782394-25da2570-550b-4b7a-85fe-219630455631.png)
Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.

## Actividades
Para la reproducci√≥n del PIPELINE se generaron dos COLAB, si bien no se pueden reproducir los procesos de automatizaci√≥n, scheduling, etc. de un PIPELINE productivo. se trat√≥ de seguir un ciclo de vida de ML orquestado similar a un ambiente productivo  
El primer COLAB abarca las Etapas de Ingesta/Visualizaci√≥n/EDA/Modelado ML 
colab https://github.com/gusper01/seminarioITBA/blob/gh-pages/seminarioITBA_EDA.ipynb
Y el segundo COLAB las Etapas de Persistencia/Recuperaci√≥n de Datos
colab https://github.com/gusper01/seminarioITBA/blob/gh-pages/seminarioITBA_PIPELINE.ipynb

Repositorio
git clone https://github.com/gusper01/seminarioITBA.git

## Etapas del PipeLine üöÄ
```
Se definieron las siguientes etapas  
```
### Ingesta de Datos üìã
```
Se toman los datasets de la competencia de Kaggle  
```
### An√°lisis Exploratorio (EDA) üîß
```
Se realizan visualizaciones de los datasets de entrenamiento y test
```
### Limpieza de Datos ‚öôÔ∏è
```
Se identifican features y se realizan actividades de limpieza e imputaci√≥n de datos
```
### Label Enconding/Baseline üî©
```
Se realizan encoding de features a los efectos de poder utilizar algoritmo XGBoost
Se genera una funci√≥n de Baseline para comparar feature importance y a los efectos de medir los modelos que se vayan desarrollando y una funcion de scoring con RMSLE (Root Mean Squared Log Error) es lo pedido en la competencia de kaggle original y es usual como m√©trica en problemas de regresi√≥n)
```

### Feature Engineering  ‚å®Ô∏è
```
Se crean nuevas posibles Features y se comparan con baseline
Se va a utilizar la m√©trica de Informacion Mutua. Esta tecnica se parece mucho a la correlaci√≥n en el sentido de que mide la relaci√≥n entre dos cantidades. La ventaja de la informaci√≥n mutua es que puede detectar cualquier tipo de relaci√≥n, mientras que la correlaci√≥n solo detecta relaciones lineales. Es f√°cil de usar e interpretar, resistente al overfitting y capaz de detectar cualquier tipo de relaci√≥n. Intuitivamente, la informaci√≥n mutua media mide la informaci√≥n que X e Y comparten: mide en cu√°nto el conocimiento de una variable reduce nuestra incertidumbre sobre la otra. Por ejemplo, si X e Y son independientes, entonces conocer X no da informaci√≥n sobre Y y viceversa, por lo que su informaci√≥n mutua es cero. En el otro extremo, si X e Y son id√©nticas entonces toda informaci√≥n proporcionada por X es compartida por Y: saber X determina el valor de Y y viceversa. Por ello, la informaci√≥n mutua media es igual a la informaci√≥n contenida en Y (o X) por s√≠ sola, tambi√©n llamada la entrop√≠a de Y (o X: claramente si X e Y son id√©nticas tienen id√©ntica entrop√≠a https://es.m.wikipedia.org/wiki/Informaci%C3%B3n_mutua
```
### Hyperparameter Tuning  üõ†Ô∏è
```
A los fines del TP esta actividad no se realiz√≥ de forma pr√°ctica pero deber√≠a estar en un ciclo de pipeline productivo ya que es clave sobre todo en los algoritmos como XGBoost. Se puede implementar con una funci√≥n que optimice los par√°metros. Los par√°metros se refieren entre otras caracter√≠sticas a: Cantidad de Hojas, Produndidad, iteracciones, tipo de validacion, escalamiento, Se puede utilizar optimizacion bayesiana para hacer la b√∫squeda de los mejores parametros. En esta caso que se utiliza XGBOOST con PYTHON se puede utilizar HYPEROT, tambi√©n se puede probar manualmente. Los algoritmos de boosting se procesan de forma secuencial para la generacion de cada arbol y en gral. requieren mucho hardaware comprometido y muchas veces es conveniente realizarlo utilizando un servicio en la nube
```
### Creaci√≥n del Modelo üì¶
```
Se genera modelo y predicciones 
Se utilizar√° solo el algoritmo XGBoost por limitaciones de tiempo. Se gener√° el archivo con la predicci√≥n para hacer el submit a Kaggle y un archivo de de salida para realizar persistencia "datatotal01102021.csv" y continuar con el PIPELINE de ML (en otro COLAB)
```
### Persistencia üî©
```
Se configura SPARK en Colab
Creaci√≥n Dataframe Spark con datos de predicci√≥n
Se genera Dataset Spark y se realizan visualizaciones de datos y schema
Se realizan "casteos" de columnas de Dataset Spark
Se genera persistencia grabando un archivo parquet
A partir de archivo parquet se realizan consultas PYSPARK SQL y se genera un filtrado de datos en base a un condicion se exporta esta salida a un Pandas Dataframe
```
#### Recuperaci√≥n de Datos üî©
```
Se visualiza extraccion de datos de archivo Parquet
```

## Construido con üõ†Ô∏è

* [Dropwizard](http://www.dropwizard.io/1.0.2/docs/) - El framework web usado
* [Maven](https://maven.apache.org/) - Manejador de dependencias
* [ROME](https://rometools.github.io/rome/) - Usado para generar RSS

## Contribuyendo üñáÔ∏è

Por favor lee el [CONTRIBUTING.md](https://gist.github.com/villanuevand/xxxxxx) para detalles de nuestro c√≥digo de conducta, y el proceso para enviarnos pull requests.

## Wiki üìñ

Puedes encontrar mucho m√°s de c√≥mo utilizar este proyecto en nuestra [Wiki](https://github.com/tu/proyecto/wiki)

## Versionado üìå

Usamos [SemVer](http://semver.org/) para el versionado. Para todas las versiones disponibles, mira los [tags en este repositorio](https://github.com/tu/proyecto/tags).

## Autor ‚úíÔ∏è

* **Gustavo Pereyra** - [gusper01](https://github.com/gusper01)üòä

## Licencia üìÑ

Este proyecto est√° bajo la Licencia (Tu Licencia) - mira el archivo [LICENSE.md](LICENSE.md) para detalles

## Expresiones de Gratitud üéÅ

* Comenta a otros sobre este proyecto üì¢
* Invita una cerveza üç∫ o un caf√© ‚òï a alguien del equipo. 
* Da las gracias p√∫blicamente ü§ì.
* etc.



---
‚å®Ô∏è con ‚ù§Ô∏è por [Villanuevand](https://github.com/Villanuevand) üòä
